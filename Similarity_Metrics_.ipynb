{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PSMfB11EDad"
      },
      "source": [
        "# Introduction \n",
        "\n",
        "In this notebook, we use 3 different similarity metrics to compare XAI saliency maps with heatmaps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5koUG0oF_Ye"
      },
      "source": [
        "# 1. Formulate similarity metric functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error # MAE\n",
        "from scipy.spatial import distance # Cosine similarity\n",
        "import numpy as np\n",
        "\n",
        "def KL_divergence(matrix1, matrix2):\n",
        "    '''\n",
        "    Adapted from https://github.com/cvzoya/saliency/blob/master/code_forMetrics/KLdiv.m\n",
        "    '''\n",
        "    # make sure each matrix sums up to 1\n",
        "    matrix1 = matrix1 / np.sum(matrix1)\n",
        "    matrix2 = matrix2 / np.sum(matrix2)\n",
        "\n",
        "    eps = np.finfo(float).eps\n",
        "    score = np.sum(np.sum(matrix2 * np.log(eps + matrix2 / (matrix1 + eps))))\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLUdOcKREZPq"
      },
      "source": [
        "# 2. Import saliency map and heatmap outputs from Google Drive\n",
        "\n",
        "Make sure all json files have been uploaded to your Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wGuiJ-UGfQC"
      },
      "source": [
        "**Note**: The code below takes in either ROI or Fixation approach heatmaps only. Therefore, you will need to change certain parts of the code and rerun everything below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from algorithms import read_tensor, get_model, get_class_name, rise, gradcam, risecam, gradrise\n",
        "image_list = []\n",
        "\n",
        "for img_name in os.listdir('Finalized Stimuli_Correct Resolution'):\n",
        "    img = read_tensor('Finalized Stimuli_Correct Resolution/{}'.format(img_name))\n",
        "\n",
        "    model = get_model()\n",
        "    p, c = torch.topk(model(img.cuda()), k=1)\n",
        "    p, c = p[0], c[0]\n",
        "\n",
        "    image_list.append(['Finalized Stimuli_Correct Resolution/{}'.format(img_name), get_class_name(c)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for img_path, predicted_calss in tqdm(image_list):\n",
        "    img_name = img_path.split('/')[-1].split('.')[0]\n",
        "    input_tensor = read_tensor(img_path)\n",
        "\n",
        "    rise_sal = rise(model, input_tensor, mask_path='masks_6000.npy')\n",
        "    gradcam_sal = gradcam(input_tensor)\n",
        "    risecam_sal_optimal = risecam(input_tensor, mask_path='masks_6000.npy', top_k='optimal')\n",
        "    risecam_sal_auto = risecam(input_tensor, mask_path='masks_6000.npy', top_k='auto')\n",
        "    risexcam_sal = np.multiply(rise_sal, gradcam_sal)\n",
        "    gradrise_sal = gradrise(input_tensor)\n",
        "\n",
        "    np.save(f'results/rise/{img_name}.npy', rise_sal)\n",
        "    np.save(f'results/gradcam/{img_name}.npy', gradcam_sal)\n",
        "    np.save(f'results/risecam_optimal/{img_name}.npy', risecam_sal_optimal)\n",
        "    np.save(f'results/risecam_auto/{img_name}.npy', risecam_sal_auto)\n",
        "    np.save(f'results/risexcam/{img_name}.npy', risexcam_sal)\n",
        "    np.save(f'results/gradrise/{img_name}.npy', gradrise_sal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KbRswbCF77Eu"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "mode = 'Fixation' # change to 'ROI' or 'Fixation' accordingly\n",
        "\n",
        "with open(f'{mode}_cat_foc.json', 'rb') as handle: \n",
        "    cat_focused = pickle.load(handle)\n",
        "with open(f'{mode}_exp_foc.json', 'rb') as handle: \n",
        "    exp_focused = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "all_maps = {'gradcam': {}, 'rise': {}, 'risecam_auto': {}, 'risecam_optimal': {}, 'risexcam': {}, 'gradrise': {}}\n",
        "for algorithm in all_maps.keys():\n",
        "    for filename in os.listdir(f'results/{algorithm}'):\n",
        "        image_name = filename.split('.')[0]\n",
        "        image_array = np.load(f'results/{algorithm}/{filename}')\n",
        "        normalized_image_array = (image_array - image_array.min()) / (image_array.max() - image_array.min())\n",
        "        all_maps[algorithm][f'{image_name}.jpg'] = normalized_image_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbBrb_5GGnSt"
      },
      "source": [
        "# 3. Generate similarity metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-tAQ0rhLvsg"
      },
      "source": [
        "## Create focused and exploratory dictionaries to store our output\n",
        "\n",
        "Sample output of each dictionary\n",
        "\n",
        "e.g. to get MAE metrics for ant2.jpg using focused strategy: \n",
        "- focused['MAE']['ant2.jpg'] = [rise_score, pcb_score, gradcam_score] *in this order*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Xwzgbdj7Ks6Z"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "focused = defaultdict(lambda: defaultdict(dict))\n",
        "explorative = defaultdict(lambda: defaultdict(dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "for image in cat_focused.keys():\n",
        "\n",
        "    cat_foc = cat_focused[image]\n",
        "    exp_foc = exp_focused[image]\n",
        "\n",
        "    focused['cat'][image] = defaultdict(list)\n",
        "    focused['exp'][image] = defaultdict(list)\n",
        "\n",
        "    for dicts in all_maps.values():\n",
        "        map = dicts[image][:224, :224]\n",
        "\n",
        "        # focused strategy\n",
        "        MAE_cat_foc = mean_absolute_error(cat_foc, map)\n",
        "        KL_cat_foc = KL_divergence(cat_foc, map)\n",
        "        cossim_cat_foc = 1 - distance.cosine(cat_foc.flatten(), map.flatten())\n",
        "\n",
        "        MAE_exp_foc = mean_absolute_error(exp_foc, map)\n",
        "        KL_exp_foc = KL_divergence(exp_foc, map)\n",
        "        cossim_exp_foc = 1 - distance.cosine(exp_foc.flatten(), map.flatten())\n",
        "\n",
        "        focused['cat'][image]['MAE'].append(MAE_cat_foc)\n",
        "        focused['cat'][image]['KL_divergence'].append(KL_cat_foc)\n",
        "        focused['cat'][image]['cos_sim'].append(cossim_cat_foc)\n",
        "\n",
        "        focused['exp'][image]['MAE'].append(MAE_exp_foc)\n",
        "        focused['exp'][image]['KL_divergence'].append(KL_exp_foc)\n",
        "        focused['exp'][image]['cos_sim'].append(cossim_exp_foc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwkIJVX1GtTT"
      },
      "source": [
        "# 4. Export metrics to CSV files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8fkFcwk-8InM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "fRRBav7L8K6w",
        "outputId": "c8f69d5f-3645-4f7d-81aa-1075b43ccf3c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>exp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ant1.jpg</th>\n",
              "      <td>{'MAE': [0.24305360150381666, 0.08854797780215...</td>\n",
              "      <td>{'MAE': [0.2082146013497567, 0.086362746072578...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ant2.jpg</th>\n",
              "      <td>{'MAE': [0.18343425941888755, 0.07634076885880...</td>\n",
              "      <td>{'MAE': [0.158592969752612, 0.0774286316582828...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ant3.jpg</th>\n",
              "      <td>{'MAE': [0.18160235269307806, 0.09705981876412...</td>\n",
              "      <td>{'MAE': [0.154010358024971, 0.0863205828260973...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ant4.jpg</th>\n",
              "      <td>{'MAE': [0.17849830958176618, 0.05038081539178...</td>\n",
              "      <td>{'MAE': [0.17002220511838742, 0.04998341555680...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ant5.jpg</th>\n",
              "      <td>{'MAE': [0.2051358438909155, 0.089157221897310...</td>\n",
              "      <td>{'MAE': [0.19953004473060335, 0.08619683710480...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        cat  \\\n",
              "ant1.jpg  {'MAE': [0.24305360150381666, 0.08854797780215...   \n",
              "ant2.jpg  {'MAE': [0.18343425941888755, 0.07634076885880...   \n",
              "ant3.jpg  {'MAE': [0.18160235269307806, 0.09705981876412...   \n",
              "ant4.jpg  {'MAE': [0.17849830958176618, 0.05038081539178...   \n",
              "ant5.jpg  {'MAE': [0.2051358438909155, 0.089157221897310...   \n",
              "\n",
              "                                                        exp  \n",
              "ant1.jpg  {'MAE': [0.2082146013497567, 0.086362746072578...  \n",
              "ant2.jpg  {'MAE': [0.158592969752612, 0.0774286316582828...  \n",
              "ant3.jpg  {'MAE': [0.154010358024971, 0.0863205828260973...  \n",
              "ant4.jpg  {'MAE': [0.17002220511838742, 0.04998341555680...  \n",
              "ant5.jpg  {'MAE': [0.19953004473060335, 0.08619683710480...  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "focusedDf = pd.DataFrame.from_dict(focused)\n",
        "focusedDf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj7wXofcHKeC"
      },
      "source": [
        "## Please run everything below *in order.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "column_names = [\n",
        "    'RISE categorization focused',\n",
        "    'GRADCAM categorization focused', \n",
        "    'RISECAM Auto categorization focused',\n",
        "    'RISECAM Optimal categorization focused',\n",
        "    'RISExCAM categorization focused',\n",
        "    'GRADRISE categorization focused',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rRKrp1N6dTq0"
      },
      "outputs": [],
      "source": [
        "foc_cat = pd.json_normalize(focusedDf['cat'])\n",
        "\n",
        "MAE_score = pd.DataFrame(foc_cat['MAE'].to_list(), columns=column_names, index=focusedDf.index)\n",
        "KL_divergence_score = pd.DataFrame(foc_cat['KL_divergence'].to_list(), columns=column_names, index=focusedDf.index)\n",
        "cos_sim_score = pd.DataFrame(foc_cat['cos_sim'].to_list(), columns=column_names, index=focusedDf.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lgttR2GrAvb_"
      },
      "outputs": [],
      "source": [
        "foc_exp = pd.json_normalize(focusedDf['exp'])\n",
        "\n",
        "MAE_score = MAE_score.merge(pd.DataFrame(foc_exp['MAE'].to_list(), columns=column_names, index=focusedDf.index), left_index=True, right_index=True)\n",
        "KL_divergence_score = KL_divergence_score.merge(pd.DataFrame(foc_exp['KL_divergence'].to_list(), columns=column_names, index=focusedDf.index), left_index=True, right_index=True)\n",
        "cos_sim_score = cos_sim_score.merge(pd.DataFrame(foc_exp['cos_sim'].to_list(), columns=column_names, index=focusedDf.index), left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RISE categorization focused_x               0.246445\n",
              "GRADCAM categorization focused_x            0.171012\n",
              "RISECAM Auto categorization focused_x       0.238297\n",
              "RISECAM Optimal categorization focused_x    0.208170\n",
              "RISExCAM categorization focused_x           0.149097\n",
              "GRADRISE categorization focused_x           0.203481\n",
              "RISE categorization focused_y               0.210730\n",
              "GRADCAM categorization focused_y            0.152567\n",
              "RISECAM Auto categorization focused_y       0.214979\n",
              "RISECAM Optimal categorization focused_y    0.187802\n",
              "RISExCAM categorization focused_y           0.124704\n",
              "GRADRISE categorization focused_y           0.171610\n",
              "dtype: float64"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MAE_score.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RISE categorization focused_x               0.988561\n",
              "GRADCAM categorization focused_x            1.094419\n",
              "RISECAM Auto categorization focused_x       1.305733\n",
              "RISECAM Optimal categorization focused_x    1.273703\n",
              "RISExCAM categorization focused_x           0.770234\n",
              "GRADRISE categorization focused_x           0.763262\n",
              "RISE categorization focused_y               0.414770\n",
              "GRADCAM categorization focused_y            0.538295\n",
              "RISECAM Auto categorization focused_y       0.658107\n",
              "RISECAM Optimal categorization focused_y    0.644091\n",
              "RISExCAM categorization focused_y           0.322460\n",
              "GRADRISE categorization focused_y           0.337100\n",
              "dtype: float64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "KL_divergence_score.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RISE categorization focused_x               0.747954\n",
              "GRADCAM categorization focused_x            0.743865\n",
              "RISECAM Auto categorization focused_x       0.729307\n",
              "RISECAM Optimal categorization focused_x    0.736507\n",
              "RISExCAM categorization focused_x           0.777602\n",
              "GRADRISE categorization focused_x           0.830619\n",
              "RISE categorization focused_y               0.829709\n",
              "GRADCAM categorization focused_y            0.806488\n",
              "RISECAM Auto categorization focused_y       0.788372\n",
              "RISECAM Optimal categorization focused_y    0.792847\n",
              "RISExCAM categorization focused_y           0.850797\n",
              "GRADRISE categorization focused_y           0.885003\n",
              "dtype: float64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cos_sim_score.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "otnViQi2FxlA"
      },
      "outputs": [],
      "source": [
        "# MAE_score.to_csv('Mean Absolute Error (MAE).csv')\n",
        "# KL_divergence_score.to_csv('KL_divergence.csv')\n",
        "# cos_sim_score.to_csv('Cosine Similarity.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Similarity_Metrics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
